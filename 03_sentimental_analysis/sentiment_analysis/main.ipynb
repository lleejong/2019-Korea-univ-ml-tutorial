{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wboTX9s9eHKw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -R sentiment_analysis\n",
        "!git clone https://github.com/KisuYang/sentiment_analysis.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PyIrJx5dUWZL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "class Data():\n",
        "    def __init__(self, args):\n",
        "        self.hparams = args.hparams\n",
        "        self.data_path = os.path.join(args.base_dir, args.data)\n",
        "        self.max_sentence_length = 0\n",
        "        self.max_word_length = 0\n",
        "\n",
        "        #word_vocab -> train.vocab\n",
        "        self.get_vocab()\n",
        "        #char_vocab\n",
        "        self.get_char_vocab()\n",
        "\n",
        "    def get_vocab(self):\n",
        "\n",
        "        #train_vocab\n",
        "        with open(os.path.join(self.data_path,\"train.vocab\"),\"r\") as f_handle:\n",
        "            self.id2word = [line.strip() for line in list(f_handle) if len(line.strip()) > 0]\n",
        "\n",
        "        self.word2id = dict()\n",
        "        for i, word in enumerate(self.id2word):\n",
        "            self.word2id[word] = i\n",
        "\n",
        "        #label.vocab\n",
        "        with open(os.path.join(self.data_path,\"label.vocab\"),\"r\") as f_handle:\n",
        "            labels = [l.strip() for l in list(f_handle) if len(l.strip()) > 0]\n",
        "        self.id2label = labels\n",
        "        self.label2id = dict()\n",
        "\n",
        "        for i, label in enumerate(labels):\n",
        "            self.label2id[label] = i\n",
        "\n",
        "    def get_char_vocab(self):\n",
        "        self.id2char = list()\n",
        "\n",
        "        with open(os.path.join(self.data_path,\"train.inputs\"),\"r\") as f_handle:\n",
        "            text = [l.strip() for l in list(f_handle) if len(l.strip()) > 0]\n",
        "            full_text = \"\"\n",
        "            for sentence in text:\n",
        "                full_text += \"\".join(sentence.split(\" \"))\n",
        "\n",
        "        alphabet_counter = collections.Counter(full_text).most_common()\n",
        "        for alphabet, count in alphabet_counter:\n",
        "            self.id2char.append(alphabet)\n",
        "\n",
        "        self.char2id = dict()\n",
        "        self.id2char.insert(0, \"<PAD>\")\n",
        "\n",
        "        for i, char in enumerate(self.id2char):\n",
        "            self.char2id[char] = i\n",
        "\n",
        "    def load_data(self, data_type=\"train\"):\n",
        "        inputs, labels, lengths = [], [], []\n",
        "\n",
        "        char_inputs, char_inputs_temp = [], []\n",
        "        char_lengths, char_lengths_temp = [], []\n",
        "\n",
        "        with open(os.path.join(self.data_path,\"%s.inputs\" % data_type),\"r\") as f_handle:\n",
        "            for i, sentence in enumerate(list(f_handle)):\n",
        "\n",
        "                inputs.append(sentence.strip().split(' '))\n",
        "                sentence_len = len(sentence.strip().split(' '))\n",
        "\n",
        "                if len(sentence.strip().split(' ')) < self.max_sentence_length:\n",
        "                    self.max_sentence_length = sentence_len\n",
        "\n",
        "                #make the list about char lengths\n",
        "                for words in sentence.strip().split(' '):\n",
        "                    char_inputs_temp.append(list(words))\n",
        "                    char_lengths_temp.append(len(list(words)))\n",
        "\n",
        "                    if len(list(words)) > self.max_word_length:\n",
        "                        self.max_word_length = len(list(words))\n",
        "\n",
        "                char_inputs.append(char_inputs_temp)\n",
        "                char_lengths.append(char_lengths_temp)\n",
        "                char_inputs_temp = []\n",
        "                char_lengths_temp = []\n",
        "\n",
        "        with open(os.path.join(self.data_path, \"%s.labels\" % data_type), \"r\") as f_handle:\n",
        "            for i, sentence in enumerate(list(f_handle)):\n",
        "                labels.append(sentence.strip().split(' '))\n",
        "\n",
        "        for sentence in inputs:\n",
        "            lengths.append(len(sentence))\n",
        "\n",
        "        return (char_inputs, char_lengths), (inputs, labels, lengths)\n",
        "\n",
        "    def data_id(self, inputs, labels, chars):\n",
        "        inputs_id = inputs\n",
        "        labels_id = labels\n",
        "        chars_id = chars\n",
        "\n",
        "        for sentence in inputs_id:\n",
        "            for i, word in enumerate(sentence):\n",
        "                try:\n",
        "                    sentence[i] = self.word2id[word]\n",
        "\n",
        "                except KeyError:\n",
        "                    sentence[i] = len(self.word2id)\n",
        "\n",
        "        for sentence in labels_id:\n",
        "            for i, label in enumerate(sentence):\n",
        "                sentence[i] = self.label2id[label]\n",
        "\n",
        "        for sentence in chars_id:\n",
        "            for i, word in enumerate(sentence):\n",
        "                for j, char in enumerate(word):\n",
        "                    try:\n",
        "                        sentence[i][j] = self.char2id[char]\n",
        "                    except KeyError:\n",
        "                        print(\"char key error : \", char)\n",
        "                        self.char2id[char] = len(self.id2char)\n",
        "                        sentence[i][j] = self.char2id[char]\n",
        "\n",
        "        return inputs_id, labels_id, chars_id\n",
        "\n",
        "    def get_batch_data(self, input_id, labels_id, train_lengths, chars_id, char_lengths, iter, batch_size):\n",
        "        idx = iter * batch_size\n",
        "        batch_inputs = input_id[idx:idx + batch_size]\n",
        "        batch_labels = labels_id[idx:idx + batch_size]\n",
        "        batch_lengths = train_lengths[idx:idx + batch_size]\n",
        "\n",
        "        batch_char_inputs = chars_id[idx:idx + batch_size]\n",
        "        batch_char_lengths = char_lengths[idx:idx + batch_size]\n",
        "\n",
        "        max_sentence_len = max(batch_lengths)\n",
        "\n",
        "        max_word_length = 0\n",
        "        for char_len_sentence in batch_char_lengths:\n",
        "            if max_word_length < max(char_len_sentence):\n",
        "                max_word_length = max(char_len_sentence)\n",
        "\n",
        "        #sentence padding\n",
        "        for sentence in batch_inputs:\n",
        "            if len(sentence) < max_sentence_len:\n",
        "                sentence.extend([0]*(max_sentence_len-len(sentence)))\n",
        "\n",
        "        #batch_char_inputs: padding\n",
        "        for words_list in batch_char_inputs:\n",
        "            if len(words_list) < max_sentence_len:\n",
        "                for i in range(max_sentence_len - len(words_list)):\n",
        "                    words_list.append([0])\n",
        "\n",
        "            for word in words_list:\n",
        "                if len(word) < max_word_length:\n",
        "                    word.extend([0]*(max_word_length - len(word)))\n",
        "\n",
        "        #batch_char_lengths: padding\n",
        "        for words_length in batch_char_lengths:\n",
        "            if len(words_length) < max_sentence_len:\n",
        "                for i in range(max_sentence_len - len(words_length)):\n",
        "                    words_length.append(0)\n",
        "\n",
        "        batch_labels_temp = list()\n",
        "        for sentence in batch_labels:\n",
        "            batch_labels_temp.extend(sentence)\n",
        "\n",
        "        batch_labels = batch_labels_temp\n",
        "\n",
        "        return batch_inputs, batch_labels, batch_lengths, batch_char_inputs, batch_char_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6v54WsJVV2uw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "import numpy as np\n",
        "\n",
        "class TextClassifier:\n",
        "    def __init__(self, args, hparams):\n",
        "        self.hparams = hparams\n",
        "        self.data_dir = args.data\n",
        "        self.eval_dir = args.eval_dir\n",
        "        self.base_dir = args.base_dir\n",
        "        #logger\n",
        "        self._logger = logging.getLogger(__name__)\n",
        "\n",
        "        #data_process\n",
        "        self.data_process = Data(args)\n",
        "        (self.char_inputs, self.char_lengths), (self.inputs, self.labels, self.lengths) = \\\n",
        "            self.data_process.load_data()\n",
        "\n",
        "        # word, id\n",
        "        self.word2id = self.data_process.word2id  # dict()\n",
        "        self.id2word = self.data_process.id2word  # vocabulary\n",
        "\n",
        "        # label, id\n",
        "        self.label2id = self.data_process.label2id\n",
        "        self.id2label = self.data_process.id2label\n",
        "\n",
        "        # pre-trained word2vec\n",
        "        with np.load(os.path.join(self.base_dir, self.hparams.glove_dir, \"glove.6B.300d.trimmed.npz\")) as pretrained_data:\n",
        "            self.word_embeddings = pretrained_data[\"embeddings\"]\n",
        "            print(np.shape(self.word_embeddings))\n",
        "\n",
        "    def _inference(self, inputs, lengths, char_inputs, char_lengths):\n",
        "        print(\"Building graph for model: Text Classifier\")\n",
        "\n",
        "        # Number of possible output cateIories.\n",
        "        output_dim = len(self.id2label) # output_dim -> 2\n",
        "\n",
        "        word_embeddings = tf.Variable(\n",
        "            self.word_embeddings,\n",
        "            name=\"word_embeddings\",\n",
        "            dtype=tf.float32,\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        ## shape = [batch_size, time, embed_dim]\n",
        "        word_embedded = tf.nn.embedding_lookup(word_embeddings, inputs)\n",
        "        word_feature_map = tf.expand_dims(word_embedded, -1)\n",
        "\n",
        "        # Convolution & Maxpool\n",
        "        features = []\n",
        "        for size in self.hparams.filter_size:\n",
        "            with tf.variable_scope(\"CNN_filter_%d\" % size):\n",
        "                # Add padding to mark the beginning and end of words.\n",
        "                pad_height = size - 1\n",
        "                pad_shape = [[0, 0], [pad_height, pad_height], [0, 0], [0, 0]]\n",
        "                word_feature_map = tf.pad(word_feature_map, pad_shape)\n",
        "                feature = tf.layers.conv2d(\n",
        "                    inputs=word_feature_map,\n",
        "                    filters=self.hparams.num_filters,\n",
        "                    kernel_size=[size, self.hparams.embedding_dim],\n",
        "                    use_bias=False\n",
        "                )\n",
        "                # shape = [batch, time, 1, out_channels]\n",
        "                feature = tf.reduce_max(feature, axis=1)\n",
        "                feature = tf.squeeze(feature)\n",
        "                feature = tf.reshape(feature, [tf.shape(inputs)[0], self.hparams.num_filters])\n",
        "                # shape = [batch, out_channels]\n",
        "                print(feature.shape)\n",
        "                self.feature_shape = tf.shape(feature)\n",
        "                # feature = tf.Print(feature, [feature], message=\"convolution feature\")\n",
        "                features.append(feature)\n",
        "\n",
        "        # shape = [batch, out_channels * num_filters]\n",
        "        layer_out = tf.concat(features, axis=1)\n",
        "        print(layer_out.shape)\n",
        "\n",
        "        with tf.variable_scope(\"layer_out\"):\n",
        "            logits = tf.layers.dense(\n",
        "                inputs=layer_out,\n",
        "                units=output_dim,\n",
        "                activation=None,\n",
        "                kernel_initializer=tf.initializers.variance_scaling(\n",
        "                    scale=2.0, mode=\"fan_in\", distribution=\"normal\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def make_placeholder(self):\n",
        "\n",
        "        self.inputs_ph = tf.placeholder(tf.int32, shape=[None, None], name=\"train_input_ph\")\n",
        "        self.labels_ph = tf.placeholder(tf.int32, shape=[None], name=\"train_label_ph\")\n",
        "        self.lengths_ph = tf.placeholder(tf.int32, shape=[None], name=\"train_lengths_ph\")\n",
        "\n",
        "        #[batch_size, word_time, char_time]\n",
        "        self.char_inputs_ph = tf.placeholder(tf.int32, shape=[None, None, None], name=\"char_input_ph\")\n",
        "        self.char_lengths_ph = tf.placeholder(tf.int32, shape=[None, None], name=\"char_lengths_ph\")\n",
        "\n",
        "        self._dropout_keep_prob_ph = tf.placeholder(tf.float32, shape=[], name=\"dropout_keep_prob\")\n",
        "\n",
        "    def make_feed_dict(self, batch_data):\n",
        "        feed_dict = {}\n",
        "        batch_inputs, batch_labels, batch_lengths, batch_char_inputs, batch_char_lengths = batch_data\n",
        "\n",
        "        # word-level\n",
        "        feed_dict[self.inputs_ph] = batch_inputs\n",
        "        feed_dict[self.labels_ph] = batch_labels\n",
        "        feed_dict[self.lengths_ph] = batch_lengths\n",
        "\n",
        "        # char-level\n",
        "        feed_dict[self.char_inputs_ph] = batch_char_inputs\n",
        "        feed_dict[self.char_lengths_ph] = batch_char_lengths\n",
        "        feed_dict[self._dropout_keep_prob_ph] = self.hparams.dropout_keep_prob\n",
        "\n",
        "        return feed_dict\n",
        "\n",
        "    def build_graph(self):\n",
        "\n",
        "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        # logits\n",
        "        with tf.variable_scope(\"inference\", reuse=False):\n",
        "            logits = self._inference(self.inputs_ph, self.lengths_ph, self.char_inputs_ph, self.char_lengths_ph)\n",
        "\n",
        "        with tf.name_scope(\"cross_entropy\"):\n",
        "            loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.labels_ph,\n",
        "                                                                     name=\"cross_entropy\")\n",
        "            self.loss_op = tf.reduce_mean(loss_op, name='cross_entropy_mean')\n",
        "            self.train_op = tf.train.AdamOptimizer().minimize(loss_op, global_step=self.global_step)\n",
        "\n",
        "        eval = tf.nn.in_top_k(logits, self.labels_ph, 1)\n",
        "        correct_count = tf.reduce_sum(tf.cast(eval, tf.int32))\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            self.accuracy = tf.divide(correct_count, tf.shape(self.labels_ph)[0])\n",
        "\n",
        "    def train(self):\n",
        "        sess = tf.Session()\n",
        "\n",
        "        with sess.as_default():\n",
        "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "            # build placeholder\n",
        "            self.make_placeholder()\n",
        "            # build train graph\n",
        "            self.build_graph()\n",
        "\n",
        "            # checkpoint file saver\n",
        "            saver = tf.train.Saver()\n",
        "\n",
        "            # get data\n",
        "            inputs_id, labels_id, chars_id = \\\n",
        "                self.data_process.data_id(self.inputs, self.labels, self.char_inputs)\n",
        "\n",
        "            total_batch = int(len(inputs_id) / self.hparams.batch_size) + 1\n",
        "            tf.global_variables_initializer().run()\n",
        "            for epochs_completed in range(self.hparams.num_epochs):\n",
        "\n",
        "                for iter in range(total_batch):\n",
        "                    batch_data = self.data_process.get_batch_data(inputs_id, labels_id, self.lengths,\n",
        "                                                                  chars_id, self.char_lengths,\n",
        "                                                                  iter, self.hparams.batch_size)\n",
        "\n",
        "                    accuracy_val, loss_val, global_step_val, _ = sess.run(\n",
        "                        [self.accuracy, self.loss_op, self.global_step, self.train_op],\n",
        "                        feed_dict=self.make_feed_dict(batch_data)\n",
        "                    )\n",
        "\n",
        "                    if global_step_val % 10 == 0:\n",
        "                        self._logger.info(\"[Step %d] loss: %.4f, accuracy: %.2f%%\" % (\n",
        "                            global_step_val, loss_val, accuracy_val * 100))\n",
        "\n",
        "                self._logger.info(\"End of epoch %d.\" % (epochs_completed + 1))\n",
        "                save_path = saver.save(sess, \"%s/model.ckpt\" % self.hparams.model, global_step=global_step_val)\n",
        "                self._logger.info(\"Model saved at: %s\" % save_path)\n",
        "            \n",
        "            \n",
        "            # evaluation\n",
        "            (self.char_inputs, self.char_lengths), (self.inputs, self.labels, self.lengths) = \\\n",
        "                self.data_process.load_data(data_type='test')        \n",
        "\n",
        "            inputs_id, labels_id, chars_id = \\\n",
        "                self.data_process.data_id(self.inputs, self.labels, self.char_inputs)\n",
        "\n",
        "            batch_data = self.data_process.get_batch_data(inputs_id, labels_id, self.lengths,\n",
        "                                                                  chars_id, self.char_lengths,\n",
        "                                                                  0, len(inputs_id))\n",
        "\n",
        "            accuracy_val, loss_val, global_step_val, _ = sess.run(\n",
        "                        [self.accuracy, self.loss_op, self.global_step, self.train_op],\n",
        "                        feed_dict=self.make_feed_dict(batch_data)\n",
        "                    )\n",
        "\n",
        "            self._logger.info(\"[Test] loss: %.4f, accuracy: %.2f%%\" % (\n",
        "                    loss_val, accuracy_val * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCxnNGSheJ3_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "import argparse\n",
        "import json\n",
        "import collections\n",
        "from datetime import datetime\n",
        "\n",
        "import logging\n",
        "\n",
        "def init_logger(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    logger = logging.getLogger()\n",
        "    logger.handlers = []\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    debug_fh = logging.FileHandler(os.path.join(path, \"debug.log\"))\n",
        "    debug_fh.setLevel(logging.DEBUG)\n",
        "\n",
        "    info_fh = logging.FileHandler(os.path.join(path, \"info.log\"))\n",
        "    info_fh.setLevel(logging.INFO)\n",
        "\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.INFO)\n",
        "\n",
        "    info_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s')\n",
        "    debug_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s | %(lineno)d:%(funcName)s')\n",
        "\n",
        "    ch.setFormatter(info_formatter)\n",
        "    info_fh.setFormatter(info_formatter)\n",
        "    debug_fh.setFormatter(debug_formatter)\n",
        "\n",
        "    logger.addHandler(ch)\n",
        "    logger.addHandler(debug_fh)\n",
        "    logger.addHandler(info_fh)\n",
        "\n",
        "    return logger\n",
        "\n",
        "def train_model(args, builder_class):\n",
        "    hparams_path = args.hparams\n",
        "\n",
        "    with open(os.path.join(args.base_dir, hparams_path), \"r\") as f_handle:\n",
        "        hparams_dict = json.load(f_handle)\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    root_dir = os.path.join(hparams_dict[\"root_dir\"], \"%s/\" % timestamp)\n",
        "\n",
        "    logger = init_logger(root_dir)\n",
        "    logger.info(\"Loaded hyper-parameter configuration from file: %s\" %hparams_path)\n",
        "    logger.info(\"Hyper-parameters: %s\" %str(hparams_dict))\n",
        "    hparams_dict[\"root_dir\"] = root_dir\n",
        "\n",
        "    hparams = collections.namedtuple(\"HParams\", sorted(hparams_dict.keys()))(**hparams_dict)\n",
        "\n",
        "    with open(os.path.join(root_dir, \"hparams.json\"), \"w\") as f_handle:\n",
        "        json.dump(hparams._asdict(), f_handle, indent=2)\n",
        "\n",
        "    # Build graph\n",
        "    model = builder_class(args, hparams)\n",
        "    model.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    class Args:\n",
        "        base_dir = 'sentiment_analysis/'\n",
        "        hparams = 'hparams/default.json'\n",
        "        data = 'amazon_reviews/'\n",
        "        eval_dir = None\n",
        "    args=Args()\n",
        "\n",
        "    train_model(args, TextClassifier)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}