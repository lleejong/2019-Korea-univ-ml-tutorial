{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_colab.py",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0DgGa8hm8mnH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **인공지능 기계학습 기초교육과정 [개체명 인식 실습] 자료 입니다.**"
      ]
    },
    {
      "metadata": {
        "id": "sjum17NTl3wv",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@ NER\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37hL-Xfu-AOP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SequenceTagger:\n",
        "    def __init__(self, hparams, data_dir):\n",
        "\n",
        "        self.hparams = hparams\n",
        "        self.data_dir = data_dir\n",
        "        self._dropout_keep_prob_ph = tf.placeholder(tf.float32, shape=[], name=\"dropout_keep_prob\")\n",
        "        self._logger = logging.getLogger(__name__)\n",
        "        self.iterator_initializers = []\n",
        "        self.idx = 0\n",
        "\n",
        "        with open(os.path.join(data_dir, \"train.inputs\"), \"r\") as _f_handle:\n",
        "            self._inputs = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n",
        "        with open(os.path.join(data_dir, \"train.labels\"), \"r\") as _f_handle:\n",
        "            self._labels = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n",
        "\n",
        "        \"\"\"\n",
        "        [A]\n",
        "        Vocabulary(단어집) 파일을 로드합니다.\n",
        "        단어 -> id, id -> 단어 변환 테이블을 생성합니다.\n",
        "        \"\"\"\n",
        "\n",
        "        with open(os.path.join(data_dir, \"train.vocab\"), \"r\") as _f_handle:\n",
        "            vocab = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n",
        "            #strip() = 한 시퀀스의 양 끝 공백 제거\n",
        "\n",
        "        #vocab_size 보다 len(vocab) 길면 vocab_size 만큼만 vocab에 넣음\n",
        "        if len(vocab) > hparams.vocab_size:\n",
        "            vocab = vocab[:hparams.vocab_size]\n",
        "        vocab.insert(len(vocab), '<UNK>')\n",
        "        self.id2word = vocab    # 리스트 : 인덱싱에 유용\n",
        "        self.word2id = {}       # {} = 딕셔너리 생성 <- 딕셔너리는 해쉬테이블을 사용 => Time complexity Big O(1)임\n",
        "        for i, word in enumerate(vocab):\n",
        "            self.word2id[word] = i\n",
        "\n",
        "        #BIO(Beginning Inside Outside)\n",
        "        #단어 -> id 생성 과정을\n",
        "\n",
        "        \"\"\"\n",
        "        [B]\n",
        "        Label(태그 모음) 파일을 로드합니다.\n",
        "        태그 -> id, id -> 태그 변환 테이블을 생성합니다.\n",
        "        \"\"\"\n",
        "\n",
        "        with open(os.path.join(data_dir, \"label.vocab\"), \"r\") as _f_handle:\n",
        "            labels = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0] #위와 동일\n",
        "\n",
        "        labels.insert(0, \"PAD\")         #단어의 0번째 자리도 PAD이므로 똑같이 맞춰줌\n",
        "        self.id2label = labels\n",
        "        self.label2id = {}              #딕셔너리 생성\n",
        "        for i, label in enumerate(labels):\n",
        "            self.label2id[label] = i\n",
        "\n",
        "    def _inference(self, inputs:tf.Tensor, lengths:tf.Tensor):\n",
        "        print(\"Building graph for model: sequence tagger\")\n",
        "\n",
        "        \"\"\"\n",
        "        [C][C]\n",
        "        단어 임베딩 행렬을 생성합니다.\n",
        "        단어 id를 단어 임베딩 텐서로 변환합니다.\n",
        "        \"\"\"\n",
        "        output_dim = len(self.id2label)\n",
        "        vocab_size = len(self.id2word) + 1\n",
        "        embeddings = tf.get_variable(\n",
        "            \"embeddings\",\n",
        "            shape=[vocab_size, self.hparams.embedding_dim],\n",
        "            initializer=tf.initializers.variance_scaling(\n",
        "                scale=1.0, mode=\"fan_out\", distribution=\"uniform\")\n",
        "        )\n",
        "\n",
        "        embedded = tf.nn.embedding_lookup(embeddings, inputs)\n",
        "        layer_out = embedded\n",
        "\n",
        "        \"\"\"\n",
        "        [D][D]\n",
        "        단어 임베딩을 RNN의 입력으로 사용하기 전,\n",
        "        차원 수를 맞춰주고 성능을 향상시키기 위해\n",
        "        projection layer를 생성하여 텐서를 통과시킵니다.\n",
        "        (high level)\n",
        "        \"\"\"\n",
        "\n",
        "        # shape dim 100 -> 128, (?, ?, 128)\n",
        "        layer_out = tf.layers.dense(\n",
        "            inputs=layer_out,\n",
        "            units=self.hparams.rnn_hidden_dim,\n",
        "            activation=tf.nn.relu,\n",
        "            kernel_initializer=tf.initializers.variance_scaling(\n",
        "                scale=1.0, mode=\"fan_avg\", distribution=\"normal\"),\n",
        "            name=\"input_projection\"\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        [E]\n",
        "        양방향 RNN을 생성하고, 여기에 텐서를 통과시킵니다.\n",
        "        이렇게 하여, 단어간 의존 관계가 반영된 단어 자질 텐서를 얻습니다.\n",
        "        \"\"\"\n",
        "        with tf.variable_scope(\"bi-RNN\"):\n",
        "            # Build RNN layers\n",
        "            rnn_cell_forward = tf.contrib.rnn.GRUCell(self.hparams.rnn_hidden_dim)\n",
        "            rnn_cell_backward = tf.contrib.rnn.GRUCell(self.hparams.rnn_hidden_dim)\n",
        "\n",
        "            # Apply dropout to RNN\n",
        "            if self.hparams.dropout_keep_prob < 1.0:\n",
        "                rnn_cell_forward = tf.contrib.rnn.DropoutWrapper(rnn_cell_forward,\n",
        "                                                                 output_keep_prob=self._dropout_keep_prob_ph)\n",
        "                rnn_cell_backward = tf.contrib.rnn.DropoutWrapper(rnn_cell_backward,\n",
        "                                                                  output_keep_prob=self._dropout_keep_prob_ph)\n",
        "\n",
        "\n",
        "            # Stack multiple layers of RNN\n",
        "            rnn_cell_forward = tf.contrib.rnn.MultiRNNCell([rnn_cell_forward] * self.hparams.rnn_depth)\n",
        "            rnn_cell_backward = tf.contrib.rnn.MultiRNNCell([rnn_cell_backward] * self.hparams.rnn_depth)\n",
        "\n",
        "            #tf.nn.bidirectional_dynamic_rnn\n",
        "            (output_forward, output_backward), _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "                rnn_cell_forward, rnn_cell_backward,\n",
        "                inputs=layer_out,\n",
        "                sequence_length=lengths,\n",
        "                dtype=tf.float32\n",
        "            )\n",
        "\n",
        "            hiddens = tf.concat([output_forward, output_backward], axis=-1)\n",
        "            # shape = [batch_size, time, rnn_dim*2]\n",
        "\n",
        "        \"\"\"\n",
        "        [F]\n",
        "        마스킹을 적용하여 문장 길이를 통일하기 위해 적용했던 padding을 제거합니다. # 마스킹 => 패딩제거\n",
        "        \"\"\"\n",
        "\n",
        "        mask = tf.sequence_mask(lengths)\n",
        "        bi_lstm_out = tf.reshape(tf.boolean_mask(hiddens, mask), [-1, self.hparams.rnn_hidden_dim * 2])\n",
        "        layer_out = bi_lstm_out\n",
        "\n",
        "        # shape=[sum of seq length, 2*LSTM hidden layer size]\n",
        "\n",
        "        \"\"\"\n",
        "        [G]\n",
        "        단어 자질 텐서를 바탕으로 단어의 태그를 예측합니다.\n",
        "        이를 위해 fully-connected(dense) layer를 생성하고 텐서를 통과시킵니다.\n",
        "        \"\"\"\n",
        "\n",
        "        #W * X + b연산 (low level)\n",
        "        with tf.variable_scope(\"read-out\"):\n",
        "            prev_layer_size = layer_out.get_shape().as_list()[-1]\n",
        "\n",
        "            weight = tf.get_variable(\"weight\", shape=[prev_layer_size, output_dim], #output_dim = 태그 개수\n",
        "                                     initializer=tf.initializers.variance_scaling(\n",
        "                                         scale=2.0, mode=\"fan_in\", distribution=\"normal\"\n",
        "                                     ))\n",
        "            bias = tf.get_variable(\"bias\", shape=[output_dim],\n",
        "                                   initializer=tf.initializers.zeros())\n",
        "            predictions = tf.add(tf.matmul(layer_out, weight), bias, name='predictions')\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def _minibatch(self, _inputs, _labels):\n",
        "\n",
        "        x_batch = []\n",
        "        y_batch = []\n",
        "        for i in range(self.hparams.batch_size):\n",
        "            if self.idx + i >= len(_inputs): break\n",
        "            x_batch.append(_inputs[self.idx + i])\n",
        "            y_batch.append(_labels[self.idx + i])\n",
        "        self.idx += self.hparams.batch_size\n",
        "        return x_batch, y_batch\n",
        "\n",
        "\n",
        "    def _load_data(self, inputs, labels=None):\n",
        "\n",
        "        x_batch = []\n",
        "        y_batch = []\n",
        "        word_arr = []\n",
        "        label_arr = []\n",
        "        label_len = []\n",
        "\n",
        "        # 전체 inputs, labels를 배치단위로 분할반복\n",
        "\n",
        "        if labels is not None:\n",
        "            x_batch, y_batch = self._minibatch(inputs, labels)\n",
        "        tokenized_inputs = []       # word_list\n",
        "        tokenized_labels = []       # label_list\n",
        "        _length = []                # length\n",
        "\n",
        "        for i in range(len(x_batch)):\n",
        "            sentence = word_tokenize(x_batch[i])\n",
        "            tokenized_inputs.append(sentence)\n",
        "            _labels = word_tokenize(y_batch[i])\n",
        "            tokenized_labels.append(_labels)\n",
        "            _length.append(len(sentence))\n",
        "            word_arr = np.zeros([len(_length), max(_length)], dtype=np.int32)\n",
        "            label_arr = np.zeros([len(_length), max(_length)], dtype=np.int32)\n",
        "\n",
        "        # word2id\n",
        "        for sent_word, sentence in enumerate(tokenized_inputs):\n",
        "            for word_id, word in enumerate(sentence):\n",
        "                if word in self.word2id:\n",
        "                    word_arr[sent_word][word_id] = self.word2id[word]\n",
        "                else:\n",
        "                    word_arr[sent_word][word_id] = self.word2id['<UNK>']  # OOV\n",
        "\n",
        "        # label2id\n",
        "        for sent_id, sentence in enumerate(tokenized_labels):\n",
        "            for label_id, word in enumerate(sentence):\n",
        "                if word in self.label2id:\n",
        "                    label_arr[sent_id][label_id] = self.label2id[word]\n",
        "\n",
        "        # label 2->1 dim\n",
        "        for i, label in enumerate(label_arr):\n",
        "            for j in range(_length[i]):\n",
        "                label_len.append(label_arr[i][j])\n",
        "\n",
        "        return word_arr, label_len, _length\n",
        "\n",
        "    def build_placeholders(self):\n",
        "        self.inputs_ph = tf.placeholder(tf.int32, shape=[None, None], name=\"inputs_ph\")\n",
        "        self.labels_ph = tf.placeholder(tf.int32, shape=[None], name=\"labels_ph\")\n",
        "        self.lengths_ph = tf.placeholder(tf.int32, shape=[None], name=\"lengths_ph\")\n",
        "\n",
        "    def train(self):\n",
        "        sess = tf.Session()\n",
        "\n",
        "        _inputs = self._inputs\n",
        "        _labels = self._labels\n",
        "\n",
        "        with sess.as_default():\n",
        "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "            self.build_placeholders()\n",
        "\n",
        "            with tf.variable_scope(\"inference\", reuse=False):\n",
        "                logits = self._inference(self.inputs_ph, self.lengths_ph)\n",
        "\n",
        "            \"\"\"\n",
        "            [O]\n",
        "            모델을 훈련시키기 위해 필요한 오퍼레이션들을 텐서 그래프에 추가합니다.\n",
        "            여기에는 loss, train, accuracy 계산 등이 포함됩니다.\n",
        "            \"\"\"\n",
        "            loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.labels_ph,\n",
        "                                                                     name=\"cross_entropy\")\n",
        "            loss_op = tf.reduce_mean(loss_op, name='cross_entropy_mean')\n",
        "            train_op = tf.train.AdamOptimizer().minimize(loss_op, global_step=global_step)\n",
        "\n",
        "            eval = tf.nn.in_top_k(logits, self.labels_ph, 1)\n",
        "            correct_count = tf.reduce_sum(tf.cast(eval, tf.int32))\n",
        "            accuracy = tf.divide(correct_count, tf.shape(self.labels_ph)[0])\n",
        "\n",
        "            # Initialize variables.\n",
        "\n",
        "            tf.global_variables_initializer().run()\n",
        "            saver = tf.train.Saver()\n",
        "\n",
        "            for epochs_completed in range(self.hparams.num_epochs):\n",
        "                for i in range(int(len(_inputs) / self.hparams.batch_size) + 1):\n",
        "\n",
        "                    \"\"\"\n",
        "                    [P]\n",
        "                    그래프에 데이터를 입력하여 필요한 계산들을 수행하고,\n",
        "                    Loss에 따라 gradient를 계산하여 파라미터들을 업데이트합니다.\n",
        "                    이러한 과정을 training step이라고 합니다.\n",
        "                    \"\"\"\n",
        "\n",
        "                    inputs, labels, lengths = self._load_data(_inputs, _labels)  # load_data = 인풋, 정답, 문장 별 길이 텐서를 만들어 줌\n",
        "\n",
        "                    try:\n",
        "                        accuracy_val, loss_val, global_step_val, _ = sess.run(\n",
        "                            [accuracy, loss_op, global_step, train_op],\n",
        "                            feed_dict={self._dropout_keep_prob_ph: self.hparams.dropout_keep_prob,\n",
        "                                       self.inputs_ph: inputs,\n",
        "                                       self.labels_ph: labels,\n",
        "                                       self.lengths_ph: lengths})\n",
        "\n",
        "                        if global_step_val % 10 == 0:\n",
        "                            print(\"[Step %d] loss: %.4f, accuracy: %.2f%%\" % (\n",
        "                            global_step_val, loss_val, accuracy_val * 100))\n",
        "                    except tf.errors.OutOfRangeError:\n",
        "                        # End of epoch.\n",
        "                        break\n",
        "                self.idx = 0\n",
        "\n",
        "                \"\"\"\n",
        "                [Q]\n",
        "                전체 학습 데이터에 대하여 1회 학습을 완료하였습니다.\n",
        "                이를 1 epoch라고 합니다.\n",
        "                딥러닝 모델의 학습은 일반적으로 수십~수백 epoch 동안 진행됩니다.\n",
        "                \"\"\"\n",
        "                self._logger.info(\"End of epoch %d.\" % (epochs_completed + 1))\n",
        "                save_path = saver.save(sess, \"NER_colab/saves/model.ckpt\", global_step=global_step_val)\n",
        "                self._logger.info(\"Model saved at: %s\" % save_path)\n",
        "\n",
        "    def predict(self, saved_file:str):\n",
        "        sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "        \"\"\"\n",
        "        [H]\n",
        "        입력 문자열을 단어/문장부호 단위로 쪼개고, 이를 다시 단어 id로 변환합니다.\n",
        "        \"\"\"\n",
        "        sentence = word_tokenize(sentence)\n",
        "        word_ids = []\n",
        "        for word in sentence:\n",
        "            if word in self.word2id:\n",
        "                word_ids.append(self.word2id[word])\n",
        "            else:\n",
        "                word_ids.append(len(self.word2id))\n",
        "\n",
        "        sess = tf.Session()\n",
        "\n",
        "        with sess.as_default():\n",
        "            \"\"\"\n",
        "            [I]\n",
        "            태깅을 수행하기 위해 텐서 그래프를 생성합니다.  python -> tensor\n",
        "            \"\"\"\n",
        "            dense_word_ids = tf.constant(word_ids)\n",
        "            lengths = tf.constant(len(word_ids))\n",
        "\n",
        "            # Insert batch dimension.\n",
        "            dense_word_ids = tf.expand_dims(dense_word_ids, axis=0)\n",
        "            lengths = tf.expand_dims(lengths, axis=0)\n",
        "\n",
        "            with tf.variable_scope(\"inference\", reuse=False):\n",
        "                logits = self._inference(dense_word_ids, lengths)\n",
        "            predictions = tf.argmax(logits, axis=1)\n",
        "\n",
        "            \"\"\"\n",
        "            [J]\n",
        "            저장된 모델을 로드하고, 데이터를 입력하여 태깅 결과를 얻습니다.\n",
        "            \"\"\"\n",
        "            saver = tf.train.Saver()\n",
        "            saver.restore(sess, saved_file)\n",
        "            pred_val = sess.run(\n",
        "                [predictions],\n",
        "                feed_dict={self._dropout_keep_prob_ph: 1.0}\n",
        "            )[0]\n",
        "\n",
        "        \"\"\"\n",
        "        [K]\n",
        "        태깅 결과를 출력합니다.\n",
        "        \"\"\"\n",
        "        pred_str = [self.id2label[i] for i in pred_val]\n",
        "        for word, tag in zip(sentence, pred_str):\n",
        "            print(\"%s[%s]\" % (word, tag), end=' ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nSsydzL0mSBG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(builder_class):\n",
        "\n",
        "    with open(\"NER_colab/default.json\", \"r\") as f_handle:\n",
        "        hparams_dict = json.load(f_handle)\n",
        "\n",
        "    hparams = collections.namedtuple(\"HParams\", sorted(hparams_dict.keys()))(**hparams_dict)\n",
        "\n",
        "    data_dir = \"NER_colab/CoNLL-2003\"\n",
        "    # Build graph\n",
        "    model = builder_class(hparams, data_dir)\n",
        "    model.train()\n",
        "    print(\"Finish Training\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wQTaT_FXmfuM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_and_predict(builder_class):\n",
        "\n",
        "    with open(\"NER_colab/default.json\", \"r\") as f_handle:\n",
        "        hparams_dict = json.load(f_handle)\n",
        "\n",
        "    hparams = collections.namedtuple(\"HParams\", sorted(hparams_dict.keys()))(**hparams_dict)\n",
        "\n",
        "    data_dir = \"NER_colab/CoNLL-2003\"\n",
        "\n",
        "    saved_dir = \"NER_colab/saves/model.ckpt-1876\"\n",
        "    model = builder_class(hparams, data_dir)\n",
        "    model.predict(saved_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fVImFrL_mkt8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    trainable = False\n",
        "    if trainable == True:\n",
        "        train_model(SequenceTagger)\n",
        "    else:\n",
        "        load_and_predict(SequenceTagger)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vifUhjh7BQu8",
        "colab_type": "code",
        "outputId": "7ea9e370-6109-4736-874e-5550be75d0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RiYKj-GNDjYb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir CoNLL-2003"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdh69Gm4DmAZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd CoNLL-2003"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OgxfQOPODzuD",
        "colab_type": "code",
        "outputId": "226c7712-0b54-4bb4-f24a-cc8c77f43cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}